{
    "docs": [
        {
            "location": "/", 
            "text": "Welcome to MZBench\n\n\nExpressive, scalable load testing tool\n\n\n\n\n\n\nMZBench\n helps software testers and developers test their products under huge load. By testing your product with MZBench before going to production, you reduce the risk of outages under real life highload. \n\n\nMZBench runs test scenarios on many machines simultaneously, maintaining millions of connections, which make it suitable even for large scale products.\n\n\nMZBench is:\n\n\n\n\nCloud-aware:\n MZBench allocates nodes directly from Amazon EC2. \n\n\nScalable:\n tested with 100 nodes and millions of connections.\n\n\nExtendable:\n write your own \ncloud plugins\n and \nworkers\n. \n\n\nOpen-source:\n MZBench is released under the \nBSD license\n.\n\n\n\n\nInstallation\n\n\nTo use MZBench, you\nll need:\n\n\n\n\nErlang R17\n\n\nC++ compiler\n\n\nPython 2.6 or 2.7 with pip\n\n\n\n\nMost UNIX systems have C++ compiler and Python preinstalled.\n\n\nErlang is available in the \nofficial repositories on most GNU/Linux distros\n. If your distro doesn\nt have Erlang R17, \nbuild it from source\n.  \n\n\nDownload MZBench from GitHub and install Python requirements:\n\n\n$ git clone https://github.com/machinezone/mzbench\n$ sudo pip install -r mzbench/requirements.txt \n\n\n\n\nQuickstart\n\n\nStart the MZBench server on localhost:\n\n\n$ cd mzbench\n$ ./bin/mzbench start_server\nExecuting make -C /path/to//mzbench/bin/../server generate\nExecuting /path/to//mzbench/bin/../server/_build/default/rel/mzbench_api/bin/mzbench_api start\n\n\n\n\n\n\nNote\n\n\nThe first server start takes a few minutes. The shell will not respond, which is OK; please be patient. Further starts will be much faster.\n\n\n\n\nWhen the server is running, launch an example benchmark:\n\n\n$ ./bin/mzbench run examples/ramp.erl\n{\n    \nstatus\n: \npending\n, \n    \nid\n: 6\n}\nstatus: running                       00:09\n\n\n\n\nGo to \nlocalhost:4800\n and see the benchmark live status:\n\n\n\n\nHow It Works\n\n\nMZBench runs your test scenarios on many \nnodes\n, simultaneously. This allows it to put extraordinarily high load on the target system\u2014we\nre talking about \nmillions\n of simultaneous connections here.\n\n\nNode\n is a machine, virtual or physical, that runs your scenarios. In real-life testing, MZBench is used with a cloud service like Amazon EC2 that provides nodes on demand. Alternatively, you can manually list the available node hosts. Anyway, you have to provide MZBench the machines to run on. If there\ns not enough nodes to run all the jobs at the same time, MZBench evenly distributes the jobs between the available nodes.\n\n\nThere\ns one node that doesn\nt run scenarios\u2014the \ndirector node\n. It collects the metrics from the other nodes and runs \npost and pre hooks\n. So, if you want to run jobs on 10 nodes, reserve 11.\n\n\n\n\nWhen the MZBench server runs your scenarios, it allocates the nodes, prepares them, and distributes the jobs. During the test run, the nodes send the collected data to the director node which then submits them to the server. The server uses the data to render graphs and show stats:\n\n\n\n\nTo know what kind of jobs MZBench can run, it\ns important to understand the concept of a \nworker\n.\n\n\nWorker\n is an Erlang module that provides functions for test scenarios. A worker may implement a common protocol like HTTP or XMPP, or a specific routine that is relevant only for a particular test case. It also implements the related metrics.\n\n\nMZBench ships with workers for \nHTTP\n and \nXMPP\n protocols and a worker that \nexecutes shell commands\n. This should be enough for most common test cases, but you can use your own workers in necessary.\n\n\nRead Next\n\n\n\n\nHow to write scenarios \u2192\n\n\nHow to control MZBench from command line \u2192\n\n\nHow to deploy MZBench \u2192\n\n\nHow to write your own worker \u2192", 
            "title": "Overview"
        }, 
        {
            "location": "/#welcome-to-mzbench", 
            "text": "Expressive, scalable load testing tool    MZBench  helps software testers and developers test their products under huge load. By testing your product with MZBench before going to production, you reduce the risk of outages under real life highload.   MZBench runs test scenarios on many machines simultaneously, maintaining millions of connections, which make it suitable even for large scale products.  MZBench is:   Cloud-aware:  MZBench allocates nodes directly from Amazon EC2.   Scalable:  tested with 100 nodes and millions of connections.  Extendable:  write your own  cloud plugins  and  workers .   Open-source:  MZBench is released under the  BSD license .", 
            "title": "Welcome to MZBench"
        }, 
        {
            "location": "/#installation", 
            "text": "To use MZBench, you ll need:   Erlang R17  C++ compiler  Python 2.6 or 2.7 with pip   Most UNIX systems have C++ compiler and Python preinstalled.  Erlang is available in the  official repositories on most GNU/Linux distros . If your distro doesn t have Erlang R17,  build it from source .    Download MZBench from GitHub and install Python requirements:  $ git clone https://github.com/machinezone/mzbench\n$ sudo pip install -r mzbench/requirements.txt", 
            "title": "Installation"
        }, 
        {
            "location": "/#quickstart", 
            "text": "Start the MZBench server on localhost:  $ cd mzbench\n$ ./bin/mzbench start_server\nExecuting make -C /path/to//mzbench/bin/../server generate\nExecuting /path/to//mzbench/bin/../server/_build/default/rel/mzbench_api/bin/mzbench_api start   Note  The first server start takes a few minutes. The shell will not respond, which is OK; please be patient. Further starts will be much faster.   When the server is running, launch an example benchmark:  $ ./bin/mzbench run examples/ramp.erl\n{\n     status :  pending , \n     id : 6\n}\nstatus: running                       00:09  Go to  localhost:4800  and see the benchmark live status:", 
            "title": "Quickstart"
        }, 
        {
            "location": "/#how-it-works", 
            "text": "MZBench runs your test scenarios on many  nodes , simultaneously. This allows it to put extraordinarily high load on the target system\u2014we re talking about  millions  of simultaneous connections here.  Node  is a machine, virtual or physical, that runs your scenarios. In real-life testing, MZBench is used with a cloud service like Amazon EC2 that provides nodes on demand. Alternatively, you can manually list the available node hosts. Anyway, you have to provide MZBench the machines to run on. If there s not enough nodes to run all the jobs at the same time, MZBench evenly distributes the jobs between the available nodes.  There s one node that doesn t run scenarios\u2014the  director node . It collects the metrics from the other nodes and runs  post and pre hooks . So, if you want to run jobs on 10 nodes, reserve 11.   When the MZBench server runs your scenarios, it allocates the nodes, prepares them, and distributes the jobs. During the test run, the nodes send the collected data to the director node which then submits them to the server. The server uses the data to render graphs and show stats:   To know what kind of jobs MZBench can run, it s important to understand the concept of a  worker .  Worker  is an Erlang module that provides functions for test scenarios. A worker may implement a common protocol like HTTP or XMPP, or a specific routine that is relevant only for a particular test case. It also implements the related metrics.  MZBench ships with workers for  HTTP  and  XMPP  protocols and a worker that  executes shell commands . This should be enough for most common test cases, but you can use your own workers in necessary.", 
            "title": "How It Works"
        }, 
        {
            "location": "/#read-next", 
            "text": "How to write scenarios \u2192  How to control MZBench from command line \u2192  How to deploy MZBench \u2192  How to write your own worker \u2192", 
            "title": "Read Next"
        }, 
        {
            "location": "/scenarios/", 
            "text": "Scenarios\n describe the behavior you want MZBench to emulate during the benchmark. If you\nre testing an online store, your scenario will probably include opening a product page and adding the product to cart. For a search service, the scenario may be searching for a random word. You get the idea.\n\n\nIn MZBench, scenarios are .erl files written in a simple \nDSL\n. It looks a lot like Erlang but is much simpler.\n\n\nRead on to learn how to write test scenarios for MZBench.\n\n\nMZBench test scenarios consist of \nlists\n, \ntuples\n, and \natoms\n:\n\n\n\n\nA \nlist\n is a comma-separated sequence enclosed in square brackets: \n[A, B, C]\n\n\nA \ntuple\n is a comma-separated sequence enclosed in curly braces: \n{A, B, C}\n\n\nAtoms\n are reserved words\u2014function names, rate units, conditions. Basically, anything that\ns not a list, a tuple, a number, or a string is an atom: \nprint, make_install, lte, rpm\n  \n\n\n\n\nThe whole scenario is a list of tuples with a dot at the end:\n\n\n[\n    {function1, param1, param2},\n    {function2, [{param_name, param_value}]},\n    ...\n].\n\n\n\n\nEach of these tuples is called a \nstatement\n. A statement represents a function call: the first item is the function name, the others are the params to pass to it. Each param in its turn can also be a list, a tuple, or an atom.\n\n\nSome statements only appear at the top level of a scenario. They\nre called \ntop-level statements\n. There\nre two kinds of top-level statements: \ndirectives\n and \npools\n.\n\n\nSee live examples of MZBench scenarios on GitHub \u2192\n\n\nDirectives\n\n\nDirectives\n prepare the system for the benchmark and clean it up after it. This includes installing an external \nworker\n on test nodes, registering resource files, checking conditions, and executing shell commands before and after the test.\n\n\nTop-Level Directives\n\n\nmake_install\n\n\n{make_install, [{git, \nURL\n}, {branch, \nBranch\n}, {dir, \nDir\n}]}\n\n\n\n\nInstall an external worker from a remote git repository on the test nodes before running the benchmark.\n\n\nMZBench downloads the worker and builds a .tgz archive, which is then distributed among the nodes and used in future provisions.\n\n\nThe following actions are executed during \nmake_install\n:\n\n\n$ git clone \nURL\n temp_dir\n$ cd temp_dir\n$ git checkout \nBranch\n\n$ cd \nDir\n\n$ make generate_tgz\n\n\n\n\nIf \nbranch\n is not specified, the default git branch is used.\n\n\nIf \ndir\n is not specified, \n.\n is used.\n\n\ninclude_resource\n\n\n{include_resource, \nResourceName\n, \nFileName\n, \nType\n}\n{include_resource, \nResourceName\n, \nFileURL\n, \nType\n}`\n\n\n\n\nRegister a \nresource file\n as \nResourceName\n.\n\n\nIf the file is on your local machine, put it in the same directory where you invoke \nmzbench run\n.\n\n\nType\n is one of the following atoms:\n\n\n\n\ntext\n\n\nPlain text file, interpreted as a single string.\n\n\njson\n\n\nJSON file. Lists are interpreted as \nErlang lists\n, objects are interpreted as \nErlang maps\n.\n\n\ntsv\n\n\nFile with tabulation separated values, interpreted as a list of lists.\n\n\nerlang\n\n\nErlang source file, interpreted directly as an \nErlang term\n.\n\n\nbinary\n\n\nCustom binary (image, executable, archive, etc.), not interpreted.\n\n\n\n\npre_hook and post_hook\n\n\n{pre_hook, \nActions\n}\n{post_hook, \nActions\n}\n\n\n\n\nRun actions before and after the benchmark. Two kinds of actions are supported: \nexec commands\n and \nworker calls\n:\n\n\nActions = [Action]\nAction = {exec, Target, BashCommand}\n    | {worker_call, WorkerMethod, WorkerModule}\n    | {worker_call, WorkerMethod, WorkerModule, WorkerType}\nTarget = all | director\n\n\n\nExec commands\n let you to run any shell command on all nodes or only on the director node.\n\n\nWorker calls\n are functions defined by the worker. They can be executed only on the director node. Worker calls are used to update the \nenvironment variables\n used in the benchmark.\n\n\nassert\n\n\n{assert, always, \nCondition\n}\n{assert, \nTime\n, \nCondition\n}\n\n\n\n\nCheck if the condition \nCondition\n is satisfied throughout the entire benchmark or at least for the amount of time \nTime\n.\n\n\nCondition\n is a comparison of two value and is defined as a tuple \n{\nOperation\n, \nOperand1\n, \nOperand2\n}\n.\n\n\nOperation\n is one of four atoms:\n\n\n\n\nlt\n\n\nLess than.\n\n\ngt\n\n\nGreater than.\n\n\nlte\n\n\nLess than or equal to.\n\n\ngte\n\n\nGreater than or equal to.\n\n\n\n\nOperand1\n and \nOperand2\n are the values to compare. They can be integers, floats, or \nmetrics\n values.\n\n\nMetrics\n are numerical values collected by the worker during the benchmark. To get the metric value, put its name between double quotation marks:\n\n\n{gt, \nhttp_ok\n, 20}\n\n\n\n\nThe \nhttp_ok\n metric is provided by the \nsimple_http\n worker. This condition passes if the number of successful HTTP responses is greater than 20.    \n\n\nPools\n\n\nPool\n represents a sequence of \njobs\n\u2014statements to run. The statements are defined by the \nworker\n and \nMZBench\ns standard library\n. The jobs are evenly distributed between nodes, so they can be executed in parallel.\n\n\nHere\ns a pool that sends HTTP GET requests to two sites on 10 nodes in parallel:\n\n\n    [ {pool,\n        [ {size, 10}, {worker_type, simple_http_worker} ],\n        [\n            {get, \nhttp://example.com\n},\n            {get, \nhttp://foobar.com\n} \n        ]\n    } ].\n\n\n\n\nThe \nget\n statement is provided by the built-in \nsimple_http\n worker.\n\n\nThe first param in the \npool\n statement is a list of \npool options\n.\n\n\nPool Options\n\n\nsize\n\n\nrequired\n\n\n{size, \nNumberOfJobs\n}`\n\n\n\n\nHow many times you want the pool executed.\n\n\nIf there\ns enough nodes and \nworker_start\n is not set, MZBench will start the jobs simultaneously and run them in parallel.\n\n\nNumberOfJobs\n is any positive number.\n\n\nworker_type\n\n\nrequired\n\n\n{worker_type, \nWorkerName\n}\n\n\n\n\nThe worker that provides statements for the jobs.\n\n\n\n\nHint\n\n\nA pool uses exactly one worker. If you need multiple workers in the benchmark, just write a pool for each one.\n\n\n\n\nworker_start\n\n\n{worker_start, {linear, \nRate\n}}\n{worker_start, {poisson, \nRate\n}}\n{worker_start, {exp, \nScale\n, \nTime\n}}\n{worker_start, {pow, \nExponent\n, \nScale\n, \nTime\n}}\n\n\n\n\nStart the jobs with a given rate:\n\n\n\n\nlinear\n\n\nConstant rate \nRate\n, e.g. 10 per minute.\n\n\npoisson\n\n\nRate defined by a \nPoisson process\n with \u03bb = \nRate\n.\n\n\nexp\n\n\n\n\nStart jobs with \nexponentially growing\n rate with the scale factor \nScale\n:\n\n\nScale \u00d7 e\nTime\n\n\n\n\npow\n\n\n\n\nStart jobs with rate growing as a \npower function\n with the exponent \nExponent\n and the scale factor \nScale\n:\n\n\nScale \u00d7 Time\nExponent\n\n\n\n\n\n\nYou can customize and combine rates:\n\n\nthink_time\n\n\n{think_time, \nTime\n, \nRate\n}\n\n\n\n\nStart jobs with rate \nRate\n for a second, then sleep for \nTime\n and repeat.\n\n\nramp\n\n\n{ramp, linear, \nStartRate\n, \nEndRate\n}\n\n\n\n\nLinearly change the rate from \nStartRate\n at the beginning of the pool to \nEndRate\n at its end.\n\n\ncomb\n\n\n{comb, \nRate1\n, \nTime1\n, \nRate2\n, \nTime2\n, ...}\n\n\n\n\nStart jobs with rate \nRate1\n for \nTime1\n, then switch to \nRate2\n for \nTime2\n, etc.\n\n\nLoops\n\n\nLoop\n is a sequence of statements executed over and over for a given time.\n\n\nA loop looks similar to a \npool\n\u2014it consists of a list of \noptions\n and a list statements to run:\n\n\n{loop, [\n        {time, \nTime\n},\n        {rate, \nRate\n},\n        {parallel, \nN\n},\n        {iterator, \nName\n},\n        {spawn, \nSpawn\n}\n    ],\n    [\n        \nStatement1\n,\n        \nStatement2\n,\n        ...\n    ]\n}\n\n\n\n\nHere\ns a loop that sends HTTP GET requests for 30 seconds with a growing rate of 1 \u2192 5 rps:\n\n\n{loop, [\n        {time, {30, sec}},\n        {rate, {ramp, linear, {1, rps}, {5, rps}}}\n    ],\n    [\n        {get, \nhttp://example.com\n}\n    ]\n}\n\n\n\n\nYou can put loops inside loops. Here\ns a nested loop that sends HTTP GET requests for 30 seconds, increasing the rate by 1 rps every three seconds:\n\n\n{loop, [\n        {time, {30, sec}},\n        {rate, {10, rpm}},\n        {iterator, \ni\n}\n    ],\n    [\n        {loop, [\n                {time, {3, sec}}, \n                {rate, {{var, \ni\n}, rps}}\n            ],\n            [\n                {get, \nhttp://google.com\n}\n            ]\n        }\n    ]\n}\n\n\n\n\nThe difference between these two examples is that in the first case the rate is growing smoothly and in the second one it\ns growing in steps.\n\n\nLoop options\n\n\ntime\n\n\nrequired\n\n\n{time, \nTime\n}\n\n\n\n\nRun the loop for \nTime\n.\n\n\nrate\n\n\n{rate, \nRate\n}\n\n\n\n\nRepeat the loop with the \nRate\n rate.\n\n\nparallel\n\n\n{parallel, \nN\n}\n\n\n\n\nRun \nN\n iterations of the loop in parallel.\n\n\niterator\n\n\n{iterator, \nIterName\n}\n\n\n\n\nDefine a variable named \nIterName\n inside the loop that contains the current iteration number. It can be accessed with \n{var, \nIterName\n}\n.\n\n\nspawn\n\n\n{spawn, (true|false)}\n\n\n\n\nIf \ntrue\n, every iteration runs in a separate, spawned process. Default is \nfalse\n.\n\n\nResource Files\n\n\nResource file\n is an external data source for the benchmark.\n\n\nTo declare a resource file for the benchmark, use \ninclude_resource\n.\n\n\nOnce the resource file is registered, its content can be included at any place in the scenario using the \nresource\n statement: \n{resource, \nResourceName\n}\n.\n\n\nFor example, suppose we have a file \nnames.json\n:\n\n\n[\n    \"Bob\",\n    \"Alice\",\n    \"Guido\"\n]\n\n\n\nHere\ns how you can use this file in a scenario:\n\n\n[\n    {include_resource, names, \nnames.json\n, json},\n    {pool, [\n            {size, 3},\n            {worker_type, dummy_worker}\n        ],\n        [\n            {loop, [\n                    {time, {5, sec}},\n                    {rate, {1, rps}}\n                ],\n                [\n                    {print, {choose, {resource, names}}} % print a random name from the file\n                ]\n            }\n        ]\n    }\n].\n\n\n\n\nStandard Library\n\n\nEnvironment Variables\n\n\nEnvironment variables\n are global values that can be accessed at any point of the benchmark. They are useful to store the benchmark global state like its total duration, or global params like the execution speed.\n\n\nTo set an environment variable, call \nmzbench\n with the \n--env\n param:\n\n\n$ ./bin/mzbench run --env foo=bar --env n=42\n\n\n\n\nvar\n\n\n{var, \nVarName\n}\n{var, \nVarName\n, \nDefaultValue\n}\n\n\n\n\nTo get the value of a variable, refer to it by the name: \n{var, \"\nVarName\n\"}\n.\n\n\n{var, \nfoo\n} % returns \nbar\n\n{var, \nn\n} % returns \n42\n, a string\n\n\n\n\nIf you refer to an undefined variable, the benchmark crashes. You can avoid this by setting a default value for the variable: \n{var, \"\nVarName\n\", \nDefaultValue\n}\n:\n\n\n{var, \nanothervar\n, \nFoo\n} % returns \nFoo\n if anothervar is not set\n\n\n\n\nnumvar\n\n\n{numvar, \nVarName\n}\n{numvar, \nVarName\n, \nDefaultValue\n}\n\n\n\n\nBy default, variable values are considered strings. To get a numerical value (integer or float), use \n{numvar, \"VarName\"}\n:\n\n\n{numvar, \nn\n} % returns 42, an integer.\n\n\n\n\nParallelization and Syncing\n\n\nparallel\n\n\n{parallel, \nStatement1\n, \nStatement2\n, ...}\n\n\n\n\nExecute multiple statements in parallel. Unlike executing statements in a pool, this way all statements are executed on the same node.\n\n\nset_signal\n\n\n{set_signal, \nSignalName\n}\n{set_signal, \nSignalName\n, \nCount\n]}\n\n\n\n\nEmit a global signal \nSignalName\n.\n\n\nIf \nCount\n is specified, the signal is emitted \nCount\n times.\n\n\nSignalName\n is a string, atom, number, or, in fact, any \nErlang term\n.\n\n\nwait_signal\n\n\n{wait_signal, \nSignalName\n}\n{wait_signal, \nSignalName\n, \nCount\n}\n\n\n\n\nWait for the global signal \nSignalName\n to be emitted. If \nCount\n is specified, wait for the signal to be emitted \nCount\n times.\n\n\nErrors Handling\n\n\nignore_failure\n\n\n{ignore_failure, \nStatement\n}\n\n\n\n\nExecute the statement \nStatement\n and continue with the benchmark even if it fails.\n\n\nIf the statement succeeds, its result is returned; otherwise, the failure reason is returned.\n\n\nRandomization\n\n\nrandom_number\n\n\n{random_number, \nMin\n, \nMax\n}\n{random_number, \nMax\n}\n\n\n\n\nReturn a random number between \nMin\n and \nMax\n, including \nMin\n and not including \nMax\n.\n\n\n{random_number, \nMax\n}\n is equivalent to \n{random_number, 0, \nMax\n}\n\n\nrandom_list\n\n\n{random_list, \nSize\n}\n\n\n\n\nReturn a list of random integer of length \nSize\n.\n\n\nrandom_binary\n\n\n{random_binary, \nSize\n}\n\n\n\n\nReturn a binary sequence of \nSize\n random bytes.\n\n\nchoose\n\n\n{choose, \nN\n, \nList\n}\n{choose, \nList\n}\n\n\n\n\nReturn a list of \nN\n random elements of the list \nList\n.\n\n\n{choose, \nList\n}\n is equivalent to \n{choose, 1, \nList\n}\n.\n\n\nround_robin\n\n\n{round_robin, \nList\n}\n\n\n\n\nPick the next element of the list. When the last one is picked, start over from the first one.\n\n\nLogging\n\n\ndump\n\n\n{dump, \nText\n}\n\n\n\n\nWrite \nText\n to the benchmark log.\n\n\nsprintf\n\n\n{sprintf, \nFormat\n, [\nValue1\n, \nValue2\n, ...]}\n\n\n\n\nReturn \nformatted text\n with a given format and placeholder values.\n\n\nData Conversion\n\n\nt\n\n\n{t, \nList\n}\n\n\n\n\nConvert \nList\n to a tuple.\n\n\nterm_to_binary\n\n\n{term_to_binary, \nterm\n}\n\n\n\n\nConvert an Erlang term to a binary object. \nLearn more\n in the Erlang docs.\n\n\nPause\n\n\nwait\n\n\n{wait, \nTime\n}\n\n\n\n\nPause the current job for \nTime\n.\n\n\nConventions\n\n\nTime\n\n\nTime\n is a tuple \n{\nDuration\n, (ms|sec|min|h)}\n:\n\n\n{1, sec}` % one second\n{10, min} % 10 minutes\n{0.5, h} % half hour\n\n\n\n\nRate\n\n\nRate\n is a tuple \n{\nN\n, (rps|rpm|rph)}\n:\n\n\n{10, rps} % 10 jobs per second\n{12, rpm} % 12 jobs per minute\n{100, h} % 100 jobs per hour", 
            "title": "Scenarios"
        }, 
        {
            "location": "/scenarios/#directives", 
            "text": "Directives  prepare the system for the benchmark and clean it up after it. This includes installing an external  worker  on test nodes, registering resource files, checking conditions, and executing shell commands before and after the test.", 
            "title": "Directives"
        }, 
        {
            "location": "/scenarios/#top-level-directives", 
            "text": "make_install  {make_install, [{git,  URL }, {branch,  Branch }, {dir,  Dir }]}  Install an external worker from a remote git repository on the test nodes before running the benchmark.  MZBench downloads the worker and builds a .tgz archive, which is then distributed among the nodes and used in future provisions.  The following actions are executed during  make_install :  $ git clone  URL  temp_dir\n$ cd temp_dir\n$ git checkout  Branch \n$ cd  Dir \n$ make generate_tgz  If  branch  is not specified, the default git branch is used.  If  dir  is not specified,  .  is used.  include_resource  {include_resource,  ResourceName ,  FileName ,  Type }\n{include_resource,  ResourceName ,  FileURL ,  Type }`  Register a  resource file  as  ResourceName .  If the file is on your local machine, put it in the same directory where you invoke  mzbench run .  Type  is one of the following atoms:   text  Plain text file, interpreted as a single string.  json  JSON file. Lists are interpreted as  Erlang lists , objects are interpreted as  Erlang maps .  tsv  File with tabulation separated values, interpreted as a list of lists.  erlang  Erlang source file, interpreted directly as an  Erlang term .  binary  Custom binary (image, executable, archive, etc.), not interpreted.   pre_hook and post_hook  {pre_hook,  Actions }\n{post_hook,  Actions }  Run actions before and after the benchmark. Two kinds of actions are supported:  exec commands  and  worker calls :  Actions = [Action]\nAction = {exec, Target, BashCommand}\n    | {worker_call, WorkerMethod, WorkerModule}\n    | {worker_call, WorkerMethod, WorkerModule, WorkerType}\nTarget = all | director  Exec commands  let you to run any shell command on all nodes or only on the director node.  Worker calls  are functions defined by the worker. They can be executed only on the director node. Worker calls are used to update the  environment variables  used in the benchmark.  assert  {assert, always,  Condition }\n{assert,  Time ,  Condition }  Check if the condition  Condition  is satisfied throughout the entire benchmark or at least for the amount of time  Time .  Condition  is a comparison of two value and is defined as a tuple  { Operation ,  Operand1 ,  Operand2 } .  Operation  is one of four atoms:   lt  Less than.  gt  Greater than.  lte  Less than or equal to.  gte  Greater than or equal to.   Operand1  and  Operand2  are the values to compare. They can be integers, floats, or  metrics  values.  Metrics  are numerical values collected by the worker during the benchmark. To get the metric value, put its name between double quotation marks:  {gt,  http_ok , 20}  The  http_ok  metric is provided by the  simple_http  worker. This condition passes if the number of successful HTTP responses is greater than 20.", 
            "title": "Top-Level Directives"
        }, 
        {
            "location": "/scenarios/#pools", 
            "text": "Pool  represents a sequence of  jobs \u2014statements to run. The statements are defined by the  worker  and  MZBench s standard library . The jobs are evenly distributed between nodes, so they can be executed in parallel.  Here s a pool that sends HTTP GET requests to two sites on 10 nodes in parallel:      [ {pool,\n        [ {size, 10}, {worker_type, simple_http_worker} ],\n        [\n            {get,  http://example.com },\n            {get,  http://foobar.com } \n        ]\n    } ].  The  get  statement is provided by the built-in  simple_http  worker.  The first param in the  pool  statement is a list of  pool options .", 
            "title": "Pools"
        }, 
        {
            "location": "/scenarios/#pool-options", 
            "text": "size  required  {size,  NumberOfJobs }`  How many times you want the pool executed.  If there s enough nodes and  worker_start  is not set, MZBench will start the jobs simultaneously and run them in parallel.  NumberOfJobs  is any positive number.  worker_type  required  {worker_type,  WorkerName }  The worker that provides statements for the jobs.   Hint  A pool uses exactly one worker. If you need multiple workers in the benchmark, just write a pool for each one.   worker_start  {worker_start, {linear,  Rate }}\n{worker_start, {poisson,  Rate }}\n{worker_start, {exp,  Scale ,  Time }}\n{worker_start, {pow,  Exponent ,  Scale ,  Time }}  Start the jobs with a given rate:   linear  Constant rate  Rate , e.g. 10 per minute.  poisson  Rate defined by a  Poisson process  with \u03bb =  Rate .  exp   Start jobs with  exponentially growing  rate with the scale factor  Scale :  Scale \u00d7 e Time   pow   Start jobs with rate growing as a  power function  with the exponent  Exponent  and the scale factor  Scale :  Scale \u00d7 Time Exponent    You can customize and combine rates:  think_time  {think_time,  Time ,  Rate }  Start jobs with rate  Rate  for a second, then sleep for  Time  and repeat.  ramp  {ramp, linear,  StartRate ,  EndRate }  Linearly change the rate from  StartRate  at the beginning of the pool to  EndRate  at its end.  comb  {comb,  Rate1 ,  Time1 ,  Rate2 ,  Time2 , ...}  Start jobs with rate  Rate1  for  Time1 , then switch to  Rate2  for  Time2 , etc.", 
            "title": "Pool Options"
        }, 
        {
            "location": "/scenarios/#loops", 
            "text": "Loop  is a sequence of statements executed over and over for a given time.  A loop looks similar to a  pool \u2014it consists of a list of  options  and a list statements to run:  {loop, [\n        {time,  Time },\n        {rate,  Rate },\n        {parallel,  N },\n        {iterator,  Name },\n        {spawn,  Spawn }\n    ],\n    [\n         Statement1 ,\n         Statement2 ,\n        ...\n    ]\n}  Here s a loop that sends HTTP GET requests for 30 seconds with a growing rate of 1 \u2192 5 rps:  {loop, [\n        {time, {30, sec}},\n        {rate, {ramp, linear, {1, rps}, {5, rps}}}\n    ],\n    [\n        {get,  http://example.com }\n    ]\n}  You can put loops inside loops. Here s a nested loop that sends HTTP GET requests for 30 seconds, increasing the rate by 1 rps every three seconds:  {loop, [\n        {time, {30, sec}},\n        {rate, {10, rpm}},\n        {iterator,  i }\n    ],\n    [\n        {loop, [\n                {time, {3, sec}}, \n                {rate, {{var,  i }, rps}}\n            ],\n            [\n                {get,  http://google.com }\n            ]\n        }\n    ]\n}  The difference between these two examples is that in the first case the rate is growing smoothly and in the second one it s growing in steps.", 
            "title": "Loops"
        }, 
        {
            "location": "/scenarios/#loop-options", 
            "text": "time  required  {time,  Time }  Run the loop for  Time .  rate  {rate,  Rate }  Repeat the loop with the  Rate  rate.  parallel  {parallel,  N }  Run  N  iterations of the loop in parallel.  iterator  {iterator,  IterName }  Define a variable named  IterName  inside the loop that contains the current iteration number. It can be accessed with  {var,  IterName } .  spawn  {spawn, (true|false)}  If  true , every iteration runs in a separate, spawned process. Default is  false .", 
            "title": "Loop options"
        }, 
        {
            "location": "/scenarios/#resource-files", 
            "text": "Resource file  is an external data source for the benchmark.  To declare a resource file for the benchmark, use  include_resource .  Once the resource file is registered, its content can be included at any place in the scenario using the  resource  statement:  {resource,  ResourceName } .  For example, suppose we have a file  names.json :  [\n    \"Bob\",\n    \"Alice\",\n    \"Guido\"\n]  Here s how you can use this file in a scenario:  [\n    {include_resource, names,  names.json , json},\n    {pool, [\n            {size, 3},\n            {worker_type, dummy_worker}\n        ],\n        [\n            {loop, [\n                    {time, {5, sec}},\n                    {rate, {1, rps}}\n                ],\n                [\n                    {print, {choose, {resource, names}}} % print a random name from the file\n                ]\n            }\n        ]\n    }\n].", 
            "title": "Resource Files"
        }, 
        {
            "location": "/scenarios/#standard-library", 
            "text": "", 
            "title": "Standard Library"
        }, 
        {
            "location": "/scenarios/#environment-variables", 
            "text": "Environment variables  are global values that can be accessed at any point of the benchmark. They are useful to store the benchmark global state like its total duration, or global params like the execution speed.  To set an environment variable, call  mzbench  with the  --env  param:  $ ./bin/mzbench run --env foo=bar --env n=42  var  {var,  VarName }\n{var,  VarName ,  DefaultValue }  To get the value of a variable, refer to it by the name:  {var, \" VarName \"} .  {var,  foo } % returns  bar \n{var,  n } % returns  42 , a string  If you refer to an undefined variable, the benchmark crashes. You can avoid this by setting a default value for the variable:  {var, \" VarName \",  DefaultValue } :  {var,  anothervar ,  Foo } % returns  Foo  if anothervar is not set  numvar  {numvar,  VarName }\n{numvar,  VarName ,  DefaultValue }  By default, variable values are considered strings. To get a numerical value (integer or float), use  {numvar, \"VarName\"} :  {numvar,  n } % returns 42, an integer.", 
            "title": "Environment Variables"
        }, 
        {
            "location": "/scenarios/#parallelization-and-syncing", 
            "text": "parallel  {parallel,  Statement1 ,  Statement2 , ...}  Execute multiple statements in parallel. Unlike executing statements in a pool, this way all statements are executed on the same node.  set_signal  {set_signal,  SignalName }\n{set_signal,  SignalName ,  Count ]}  Emit a global signal  SignalName .  If  Count  is specified, the signal is emitted  Count  times.  SignalName  is a string, atom, number, or, in fact, any  Erlang term .  wait_signal  {wait_signal,  SignalName }\n{wait_signal,  SignalName ,  Count }  Wait for the global signal  SignalName  to be emitted. If  Count  is specified, wait for the signal to be emitted  Count  times.", 
            "title": "Parallelization and Syncing"
        }, 
        {
            "location": "/scenarios/#errors-handling", 
            "text": "ignore_failure  {ignore_failure,  Statement }  Execute the statement  Statement  and continue with the benchmark even if it fails.  If the statement succeeds, its result is returned; otherwise, the failure reason is returned.", 
            "title": "Errors Handling"
        }, 
        {
            "location": "/scenarios/#randomization", 
            "text": "random_number  {random_number,  Min ,  Max }\n{random_number,  Max }  Return a random number between  Min  and  Max , including  Min  and not including  Max .  {random_number,  Max }  is equivalent to  {random_number, 0,  Max }  random_list  {random_list,  Size }  Return a list of random integer of length  Size .  random_binary  {random_binary,  Size }  Return a binary sequence of  Size  random bytes.  choose  {choose,  N ,  List }\n{choose,  List }  Return a list of  N  random elements of the list  List .  {choose,  List }  is equivalent to  {choose, 1,  List } .  round_robin  {round_robin,  List }  Pick the next element of the list. When the last one is picked, start over from the first one.", 
            "title": "Randomization"
        }, 
        {
            "location": "/scenarios/#logging", 
            "text": "dump  {dump,  Text }  Write  Text  to the benchmark log.  sprintf  {sprintf,  Format , [ Value1 ,  Value2 , ...]}  Return  formatted text  with a given format and placeholder values.", 
            "title": "Logging"
        }, 
        {
            "location": "/scenarios/#data-conversion", 
            "text": "t  {t,  List }  Convert  List  to a tuple.  term_to_binary  {term_to_binary,  term }  Convert an Erlang term to a binary object.  Learn more  in the Erlang docs.", 
            "title": "Data Conversion"
        }, 
        {
            "location": "/scenarios/#pause", 
            "text": "wait  {wait,  Time }  Pause the current job for  Time .", 
            "title": "Pause"
        }, 
        {
            "location": "/scenarios/#conventions", 
            "text": "", 
            "title": "Conventions"
        }, 
        {
            "location": "/scenarios/#time_1", 
            "text": "Time  is a tuple  { Duration , (ms|sec|min|h)} :  {1, sec}` % one second\n{10, min} % 10 minutes\n{0.5, h} % half hour", 
            "title": "Time"
        }, 
        {
            "location": "/scenarios/#rate_1", 
            "text": "Rate  is a tuple  { N , (rps|rpm|rph)} :  {10, rps} % 10 jobs per second\n{12, rpm} % 12 jobs per minute\n{100, h} % 100 jobs per hour", 
            "title": "Rate"
        }, 
        {
            "location": "/cli/", 
            "text": "The \nMZBench CLI\n lets you control you control the server and benchmarks from the command line. It utilizes the \nMZBench API\n, but goes beyond it: it can do things even without a running MZBench server.\n\n\nThe commands are invoked by the \nmzbench\n script in the \nbin\n directory:\n\n\n# Inside the MZBench directory:\n$ ./bin/mzbench start --env foo=bar --nodes=5\n# or:\n$ cd bin\n$ mzbench start --env foo=bar --nodes=5\n\n\n\n\nCommands\n\n\nServer Control\n\n\nstart_server\n\n\n$ ./bin/mzbench start_server\nExecuting make -C /path/to/mzbench/bin/../server generate\n...\n\n\n\n\nStart the MZBench server.\n\n\nOptional params:\n\n\n\n\n--config \nconfig_file\n\n\nPath to the \nserver config\n file.\n\n\n\n\nstop_server\n\n\n$ ./bin/mzbench stop_server\nExecuting make -C /path/to/mzbench/bin/../server generate\n...\n\n\n\n\nStop the MZBench server.\n\n\nrestart_server\n\n\n$ ./bin/mzbench restart_server\nExecuting make -C /path/to/mzbench/bin/../server generate\n...\n\n\n\n\nRestart the MZBench server, i.e. \nstop\n + \nstart\n.\n\n\nOptional params:\n\n\n\n\n--config \nconfig_file\n\n\nPath to the \nserver config\n file.\n\n\n\n\nBenchmark Control\n\n\nstart\n\n\n$ mzbench start --env foo=bar --nodes=5 foo.erl\n{\n    \nstatus\n: \npending\n, \n    \nid\n: 86\n}\n\n\n\n\nStart the benchmark from the given scenario file.\n\n\nPositional param:\n\n\n\n\nscenario_file\n\n\nThe path to the \nscenario\n file for the benchmark.\n\n\n\n\nOptional params:\n\n\n\n\n--name \nbenchmark_name\n\n\nBenchmark name.\n\n\n--nodes \nnodes\n\n\nNumber of nodes or a comma-separated list of node hostnames to run the benchmark on.\n\n\n--nodes_file \nfilename\n\n\nPath to a file with node hostnames separated by newlines.\n\n\n--env \nname=value\n ...\n\n\nEnvironment variable\n definitions.\n\n\n--cloud \ncloud_provider_name\n\n\nName of the cloud provider from the \nserver config\n. If not specified, the first one on the list is used.\n\n\n--email \nemail\n ...\n\n\nEmails for notifications. When the benchmark finishes, the results will be sent to these emails.\n\n\n--deallocate_after_bench false\n\n\nSkip node deallocation after the benchmark.\n\n\n--provision_nodes false\n\n\nSkip MZBench installation on the nodes.\n\n\n--exclusive_node_usage false\n\n\nAllow multiple nodes to be hosted on the same physical machine, i.e. do not allocate a physical machine exclusively for each node.\n\n\n--node_commit=\ncommit\n\n\nCommit hash or branch name in the MZBench repository pointing to the MZBench version to install on the nodes.\n\n\n\n\nrun\n\n\n$ mzbench run --env foo=bar --nodes=5 foo.erl\n{\n    \nstatus\n: \npending\n, \n    \nid\n: 86\n}\n\n\n\n\nSame as \nstart\n, but blocks until the benchmark is complete.\n\n\nPositional param:\n\n\n\n\nscenario_file\n\n\nThe path to the \nscenario\n file for the benchmark.\n\n\n\n\nrun_local\n\n\n$ mzbench run_local --env foo=bar foo.erl\nExecuting make -C /path/to/mzbench/bin/../node compile\n...\n\n\n\n\nRun the benchmark without a server. The logs are printed to stdout.\n\n\nPositional param:\n\n\n\n\nscenario_file\n\n\nThe path to the \nscenario\n file for the benchmark.\n\n\n\n\nOptional param:\n\n\n\n\n--env \nname=value\n ...\n\n\nEnvironment variable\n definitions.\n\n\n\n\nvalidate\n\n\n$ mzbench validate foo.erl\nok\n\n\n\n\nValidate the scenario file without executing it.\n\n\nPositional param:\n\n\n\n\nscenario_file\n\n\nThe path to the \nscenario\n file for the benchmark.\n\n\n\n\nstatus\n\n\n$ mzbench status 86\n{\n    \nstatus\n: \nprovisioning\n, \n    \nstart_time\n: \n2015-11-18T13:52:04Z\n\n}\n\n\n\n\nGet the status, start time, and, when completed, finish time of the benchmark.\n\n\nPositional param:\n\n\n\n\nbenchmark_id\n\n\nThe ID of the benchmark as returned by \nstart\n or \nrun\n.\n\n\n\n\nstop\n\n\n$ ./bin/mzbench stop 89\n{\n    \nstatus\n: \nstopped\n\n}\n\n\n\n\nStop the benchmark.\n\n\nPositional param:\n\n\n\n\nbenchmark_id\n\n\nThe ID of the benchmark as returned by \nstart\n or \nrun\n.\n\n\n\n\nlog\n\n\n$ ./bin/mzbench log 89\nStart of log for bench 89\n13:52:05.001 [info] [ API ] Node repo: {git_install_spec,\n...\n\n\n\n\nView the benchmark logs.\n\n\nPositional param:\n\n\n\n\nbenchmark_id\n\n\nThe ID of the benchmark as returned by \nstart\n or \nrun\n.\n\n\n\n\ndata\n\n\n$ ./bin/mzbench data 89\n[\n    {\n        \ntarget\n: \nworkers.pool1.km.ended.rps.value\n, \n        \ndatapoints\n: [\n...\n\n\n\n\nView the metrics data collected during the benchmark.\n\n\nPositional param:\n\n\n\n\nbenchmark_id\n\n\nThe ID of the benchmark as returned by \nstart\n or \nrun\n.\n\n\n\n\nchange_env\n\n\n$ mzbench change_env 86 --env foo=baz\n{\n    \nstatus\n: \nset\n\n}\n\n\n\n\nRedefine an environment variable without interrupting the benchmark:\n\n\nPositional param:\n\n\n\n\nbenchmark_id\n\n\nThe ID of the benchmark as returned by \nstart\n or \nrun\n.\n\n\n\n\nOptional param:\n\n\n\n\n--env \nname=value\n ...\n\n\nEnvironment variable\n definitions.\n\n\n\n\nMisc\n\n\nselfcheck\n\n\n$ mzbench selfcheck\nExecuting /path/to/mzbench/bin/lint.py /path/to/mzbench/bin/../\n\n\n\n\nRun the tests on MZBench.\n\n\nlist_templates\n\n\n$ mzbench list_templates\namqp\nempty\npython_empty\ntcp\n\n\n\n\nList the available worker templates to base \nnew workers\n on.\n\n\nnew_worker\n\n\n$ mzbench new_worker --template python_empty bar\nnew worker bar has been created\n\n\n\n\nCreate a new worker directory for \ndevelopment purposes\n.\n\n\nPositional param:\n\n\n\n\nworker_name\n\n\nThe name of the new worker.\n\n\n\n\nOptional params:\n\n\n\n\n--template \ntemplate_name\n\n\nThe template for the new worker. See the full list of available templates with \nlist_templates\n.", 
            "title": "CLI"
        }, 
        {
            "location": "/cli/#commands", 
            "text": "", 
            "title": "Commands"
        }, 
        {
            "location": "/cli/#server-control", 
            "text": "start_server  $ ./bin/mzbench start_server\nExecuting make -C /path/to/mzbench/bin/../server generate\n...  Start the MZBench server.  Optional params:   --config  config_file  Path to the  server config  file.   stop_server  $ ./bin/mzbench stop_server\nExecuting make -C /path/to/mzbench/bin/../server generate\n...  Stop the MZBench server.  restart_server  $ ./bin/mzbench restart_server\nExecuting make -C /path/to/mzbench/bin/../server generate\n...  Restart the MZBench server, i.e.  stop  +  start .  Optional params:   --config  config_file  Path to the  server config  file.", 
            "title": "Server Control"
        }, 
        {
            "location": "/cli/#benchmark-control", 
            "text": "start  $ mzbench start --env foo=bar --nodes=5 foo.erl\n{\n     status :  pending , \n     id : 86\n}  Start the benchmark from the given scenario file.  Positional param:   scenario_file  The path to the  scenario  file for the benchmark.   Optional params:   --name  benchmark_name  Benchmark name.  --nodes  nodes  Number of nodes or a comma-separated list of node hostnames to run the benchmark on.  --nodes_file  filename  Path to a file with node hostnames separated by newlines.  --env  name=value  ...  Environment variable  definitions.  --cloud  cloud_provider_name  Name of the cloud provider from the  server config . If not specified, the first one on the list is used.  --email  email  ...  Emails for notifications. When the benchmark finishes, the results will be sent to these emails.  --deallocate_after_bench false  Skip node deallocation after the benchmark.  --provision_nodes false  Skip MZBench installation on the nodes.  --exclusive_node_usage false  Allow multiple nodes to be hosted on the same physical machine, i.e. do not allocate a physical machine exclusively for each node.  --node_commit= commit  Commit hash or branch name in the MZBench repository pointing to the MZBench version to install on the nodes.   run  $ mzbench run --env foo=bar --nodes=5 foo.erl\n{\n     status :  pending , \n     id : 86\n}  Same as  start , but blocks until the benchmark is complete.  Positional param:   scenario_file  The path to the  scenario  file for the benchmark.   run_local  $ mzbench run_local --env foo=bar foo.erl\nExecuting make -C /path/to/mzbench/bin/../node compile\n...  Run the benchmark without a server. The logs are printed to stdout.  Positional param:   scenario_file  The path to the  scenario  file for the benchmark.   Optional param:   --env  name=value  ...  Environment variable  definitions.   validate  $ mzbench validate foo.erl\nok  Validate the scenario file without executing it.  Positional param:   scenario_file  The path to the  scenario  file for the benchmark.   status  $ mzbench status 86\n{\n     status :  provisioning , \n     start_time :  2015-11-18T13:52:04Z \n}  Get the status, start time, and, when completed, finish time of the benchmark.  Positional param:   benchmark_id  The ID of the benchmark as returned by  start  or  run .   stop  $ ./bin/mzbench stop 89\n{\n     status :  stopped \n}  Stop the benchmark.  Positional param:   benchmark_id  The ID of the benchmark as returned by  start  or  run .   log  $ ./bin/mzbench log 89\nStart of log for bench 89\n13:52:05.001 [info] [ API ] Node repo: {git_install_spec,\n...  View the benchmark logs.  Positional param:   benchmark_id  The ID of the benchmark as returned by  start  or  run .   data  $ ./bin/mzbench data 89\n[\n    {\n         target :  workers.pool1.km.ended.rps.value , \n         datapoints : [\n...  View the metrics data collected during the benchmark.  Positional param:   benchmark_id  The ID of the benchmark as returned by  start  or  run .   change_env  $ mzbench change_env 86 --env foo=baz\n{\n     status :  set \n}  Redefine an environment variable without interrupting the benchmark:  Positional param:   benchmark_id  The ID of the benchmark as returned by  start  or  run .   Optional param:   --env  name=value  ...  Environment variable  definitions.", 
            "title": "Benchmark Control"
        }, 
        {
            "location": "/cli/#misc", 
            "text": "selfcheck  $ mzbench selfcheck\nExecuting /path/to/mzbench/bin/lint.py /path/to/mzbench/bin/../  Run the tests on MZBench.  list_templates  $ mzbench list_templates\namqp\nempty\npython_empty\ntcp  List the available worker templates to base  new workers  on.  new_worker  $ mzbench new_worker --template python_empty bar\nnew worker bar has been created  Create a new worker directory for  development purposes .  Positional param:   worker_name  The name of the new worker.   Optional params:   --template  template_name  The template for the new worker. See the full list of available templates with  list_templates .", 
            "title": "Misc"
        }, 
        {
            "location": "/api/", 
            "text": "The \nMZBench API\n lets you run benchmarks on and collect data from a remote MZBench server with HTTP requests.\n\n\nThe API accepts POST and GET requests and returns data in JSON format or plaintext. In case of an error, the response is \n{\"reason\": \"Error description\", \"reason_code\": \"short_textual_id\"}\n:\n\n\n$ curl http://mzbench.myserver.com/status?id=1\n{\n    \nfinish_time\n: \n2015-11-03T13:41:27Z\n,\n    \nstart_time\n: \n2015-11-03T13:39:21Z\n,\n    \nstatus\n: \ncomplete\n\n}\n\n$ curl http://mzbench.myserver.com/logs?id=100500\n{\n    \nreason\n: \nBenchmark 100500 is not found\n,\n    \nreason_code\n: \nnot_found\n\n}\n\n\n\n\n\n\nImportant\n\n\nIf you have \nlogs\n or \nmetrics compression\n enabled in the server config, make sure you can handle compressed responses from the MZBench API.\n\n\n\n\nEndpoints\n\n\nPOST /start\n\n\nAsk the server to start a benchmark from the given \nscenario file\n. The file is submitted as form data. A successful response is a JSON object with \nid\n and \nstatus\n fields:\n\n\n# Start a benchmark from scenario.erl:\n$ curl -XPOST --form bench=@scenario.erl http://mzbench.myserver.com/start\n{\n    \nid\n: 46,\n    \nstatus\n: \npending\n\n}\n\n\n\n\nOptional query parameters:\n\n\n\n\nbenchmark_name\n\n\nThe name of the benchmark.\n\n\nnodes\n\n\n\n\nThe number of nodes to be allocated or a comma-separated list of node hostnames or IP addresses.\n\n\nDefault is \n1\n.\n\n\n\n\ndeallocate_after_bench=false\n\n\nPass to cancel node deallocation after the benchmark has finished.\n\n\nprovision_nodes=false\n\n\nPass to skip MZBench installation on the nodes.\n\n\nnode_commit\n\n\nCommit hash or branch name in the \nMZBench repository\n that should be installed on the nodes.\n\n\n\n\nExamples:\n\n\n# Start scenario.erl on five cloud nodes:\ncurl -XPOST --form bench=@scenario.erl http://mzbench.myserver.com/start?nodes=5\n\n# Start scenario.erl on three preallocated nodes 123.45.67.89 and node.myserver.com:\ncurl -XPOST --form bench=@scenario.erl http://mzbench.myserver.com/start?nodes=123.45.67.89,node.myserver.com\n\n# Start scenario.erl and disable cloud nodes deallocation:\ncurl -XPOST --form bench=@scenario.erl http://mzbench.myserver.com/start?deallocate_after_bench=false\n\n\n\n\nGET /status?id=number\n\n\nRequest the benchmark status. A successful response is a JSON object with the fields \nstatus\n, \nstart_time,\n and \nfinish_time\n:\n\n\n$ curl 'http://mzbench.myserver.com/status?id=122'\n{\n    \nstatus\n:\ncomplete\n,\n    \nstart_time\n:\n2015-08-12T07:25:42Z\n,\n    \nfinish_time\n:\n2015-08-12T07:26:29Z\n\n}\n\n\n\n\nGET /stop?id=number\n\n\nStop the benchmark. In case of success, the response JSON object will contain one field, \nstatus\n:\n\n\n$ curl 'http://mzbench.myserver.com/stop?id=122'\n{\n    \nstatus\n: \nstopped\n\n}\n\n\n\n\nGET /restart?id=number\n\n\nClone and start a previously executed benchmark. The response is the same as for the \nstart\n endpoint:\n\n\n$ curl http://mzbench.myserver.com/restart?id=122\n{\n    \nid\n: 123,\n    \nstatus\n: \npending\n\n}\n\n\n\n\nGET /logs?id=number\n\n\nRequest benchmark logs. The response is plaintext.\n\n\nIf the benchmark is still running, the logs will be streamed continuously until it finishes.\n\n\n$ curl http://mzbench.myserver.com/logs?id=122\n12:17:16.000 [info] [ API ] Node repo: {git_install_spec,\n                                        \nhttps://github.com/machinezone/mzbench.git\n,\n                                        \n2729662cb1a393f66b84b25b27f58190afd43e85\n,\n                                        \nnode\n}\n...\n\n\n\n\nGET /data?id=number\n\n\nRequest benchmark metrics data. The response will be tab-delimited CSV with timestamp followed by metric name and value:\n\n\n$ curl http://mzbench.myserver.com/data?id=1236\n1439245024  mzb.workers.failed.value    0\n1439245024  mzb.workers.failed.rps.value    0.0\n1439245024  mzb.workers.started.value   2000\n\n\n\n\nMetric data for running benches is streamed similarly to \nlogs\n.\n\n\nConventions\n\n\n\n\nBenchmark ID is a non-negative integer.\n\n\nStatus is a string: \npending\n, \nrunning\n, \ncomplete\n, \nfailed\n, or \nstopped\n.\n\n\nDate is a string in ISO 8601 format: \n2015-08-12T07:25:42Z\n.", 
            "title": "API"
        }, 
        {
            "location": "/api/#endpoints", 
            "text": "", 
            "title": "Endpoints"
        }, 
        {
            "location": "/api/#post-start", 
            "text": "Ask the server to start a benchmark from the given  scenario file . The file is submitted as form data. A successful response is a JSON object with  id  and  status  fields:  # Start a benchmark from scenario.erl:\n$ curl -XPOST --form bench=@scenario.erl http://mzbench.myserver.com/start\n{\n     id : 46,\n     status :  pending \n}  Optional query parameters:   benchmark_name  The name of the benchmark.  nodes   The number of nodes to be allocated or a comma-separated list of node hostnames or IP addresses.  Default is  1 .   deallocate_after_bench=false  Pass to cancel node deallocation after the benchmark has finished.  provision_nodes=false  Pass to skip MZBench installation on the nodes.  node_commit  Commit hash or branch name in the  MZBench repository  that should be installed on the nodes.   Examples:  # Start scenario.erl on five cloud nodes:\ncurl -XPOST --form bench=@scenario.erl http://mzbench.myserver.com/start?nodes=5\n\n# Start scenario.erl on three preallocated nodes 123.45.67.89 and node.myserver.com:\ncurl -XPOST --form bench=@scenario.erl http://mzbench.myserver.com/start?nodes=123.45.67.89,node.myserver.com\n\n# Start scenario.erl and disable cloud nodes deallocation:\ncurl -XPOST --form bench=@scenario.erl http://mzbench.myserver.com/start?deallocate_after_bench=false", 
            "title": "POST /start"
        }, 
        {
            "location": "/api/#get-statusidnumber", 
            "text": "Request the benchmark status. A successful response is a JSON object with the fields  status ,  start_time,  and  finish_time :  $ curl 'http://mzbench.myserver.com/status?id=122'\n{\n     status : complete ,\n     start_time : 2015-08-12T07:25:42Z ,\n     finish_time : 2015-08-12T07:26:29Z \n}", 
            "title": "GET /status?id=number"
        }, 
        {
            "location": "/api/#get-stopidnumber", 
            "text": "Stop the benchmark. In case of success, the response JSON object will contain one field,  status :  $ curl 'http://mzbench.myserver.com/stop?id=122'\n{\n     status :  stopped \n}", 
            "title": "GET /stop?id=number"
        }, 
        {
            "location": "/api/#get-restartidnumber", 
            "text": "Clone and start a previously executed benchmark. The response is the same as for the  start  endpoint:  $ curl http://mzbench.myserver.com/restart?id=122\n{\n     id : 123,\n     status :  pending \n}", 
            "title": "GET /restart?id=number"
        }, 
        {
            "location": "/api/#get-logsidnumber", 
            "text": "Request benchmark logs. The response is plaintext.  If the benchmark is still running, the logs will be streamed continuously until it finishes.  $ curl http://mzbench.myserver.com/logs?id=122\n12:17:16.000 [info] [ API ] Node repo: {git_install_spec,\n                                         https://github.com/machinezone/mzbench.git ,\n                                         2729662cb1a393f66b84b25b27f58190afd43e85 ,\n                                         node }\n...", 
            "title": "GET /logs?id=number"
        }, 
        {
            "location": "/api/#get-dataidnumber", 
            "text": "Request benchmark metrics data. The response will be tab-delimited CSV with timestamp followed by metric name and value:  $ curl http://mzbench.myserver.com/data?id=1236\n1439245024  mzb.workers.failed.value    0\n1439245024  mzb.workers.failed.rps.value    0.0\n1439245024  mzb.workers.started.value   2000  Metric data for running benches is streamed similarly to  logs .", 
            "title": "GET /data?id=number"
        }, 
        {
            "location": "/api/#conventions", 
            "text": "Benchmark ID is a non-negative integer.  Status is a string:  pending ,  running ,  complete ,  failed , or  stopped .  Date is a string in ISO 8601 format:  2015-08-12T07:25:42Z .", 
            "title": "Conventions"
        }, 
        {
            "location": "/deployment/", 
            "text": "A complete MZBench installation consists of two parts:\n\n\n\n\nMZBench API server that accepts client requests and provides the dashboard.\n\n\nA set of hosts operating as a worker nodes.\n\n\n\n\nOnce launched, the API server is permanent, whereas the worker nodes are dynamically\nallocated on demand on AWS or another cloud provider.\n\n\nHere\ns how you install and configure MZBench for real-life use.\n\n\nInstallation\n\n\nRequirements:\n\n\n\n\nErlang R17\n\n\nC++ compiler (preinstalled on most UNIX-based systems)\n\n\nPython\n 2.6 or 2.7\n\n\nPip\n Python package manager\n\n\n\n\nInstall and start the MZBench API server:\n\n\n# Clone the current MZBench source code:\ngit clone https://github.com/machinezone/mzbench.git\n\n# Install Python requirements:\nsudo pip install -r mzbench/requirements.txt\n\n# Start the server\n./mzbench/bin/mzbench start_server\n\n\n\n\nConfiguration\n\n\n\n\nNote\n\n\nEvery time you update the configuration, \nrestart the server\n.\n\n\n\n\nFormat\n\n\nThe MZBench server parameters are defined in the configuration file.\n\n\nBy default, the server tries to load configuration from \n~/.config/mzbench/server.config\n and \n/etc/mzbench/server.config\n.\n\n\nTo specify a configuration file from a different location, use the \n--config\n param:\n\n\n$ ./bin/mzbench start_server --config /path/to/server.config\n\n\n\n\nThe configuration file is an Erlang list with the \nmzbench_api\n tuple. This tuple holds the list of the server \nparameters\n:\n\n\n[\n    {mzbench_api, [\n        {network_interface, \n127.0.0.1\n},\n        {listen_port, 80}\n    ]}\n].\n\n\n\n\nHere, two parameters are specified: \nnetwork_interface\n and \nlisten_port\n.\n\n\nAll parameters are optional.\n\n\nParameters\n\n\ncloud_plugins\n\n\n{cloud_plugins: [\n    {\nPluginName\n, #{module =\n \nModuleName\n,\n                    \nOption1\n =\n \nValue1\n,\n                    \nOption2\n =\n \nValue2\n,\n                    ...\n                    }\n    },\n    ...\n    ]\n}\n\n\n\n\nList of \ncloud plugins\n that can be used to allocate nodes. One plugin can be listed multiple times with different settings and under different names.\n\n\nPluginName\n is an atom identifying a plugin instance.\n\n\nModuleName\n is the name of the plugin module. Each module has its specific \nOptions\n. \n\n\nThere are four built-in plugins:\n\n\n\n\nmzb_api_ec2_plugin\n\n\nAllocate hosts from the Amazon EC2 cloud.\n\n\nmzb_staticcloud_plugin\n\n\nAllocates hosts from a static pool.\n\n\nmzb_dummycloud_plugin\n\n\nDummy provider, doesn\nt really do anything, but can be used as a reference during \ncloud plugin development\n.\n\n\nmzb_multicloud_plugin\n\n\nAllocate hosts from multiple sources with the given ratio.\n\n\n\n\nnetwork_interface\n\n\n{network_interface, \nip address\n}\n\n\n\n\nSpecify the IP address for the dashboard to run on. By default it\ns \n\"127.0.0.1\"\n, so the dashboard is unavailable for external connections. \n\n\nTo open the dashboard to the public, set this param to \n\"0.0.0.0\"\n.\n\n\n\n\nWarning\n\n\nMZBench provides no authentication. Opening the dashboard to the public makes your server vulnerable.\n\n\nTo protect your server, use an external auth proxy server like Nginx.\n\n\n\n\nlisten_port\n\n\n{listen_port, \nport\n}\n\n\n\n\nSpecify the port to access the dashboard.\n\n\nDefault value: \n4800\n.\n\n\nbench_log_file\n\n\n{bench_log_file, \nfilename\n}\n\n\n\n\nThe name of the benchmark log files. The files are stored in \nbench_data_dir\n/\nbench_id\n where \nbench_data_dir\n is defined in the \nbench_data_dir\n param and \nid\n is the ID of a particular benchmark.\n\n\nDefault value: \n\"log.txt\"\n\n\nbench_log_compression\n\n\n{bench_log_compression, (deflate|none)}\n\n\n\n\nEnable or disable log compression with the \nDEFLATE\n algorithm.\n\n\nDefault value: \ndeflate\n\n\nbench_metrics_file\n\n\n{bench_metrics_file, \nfilename\n}\n\n\n\n\nThe name of the benchmark metrics data files. The files are stored in \nbench_data_dir\n/\nbench_id\n where \nbench_data_dir\n is defined in the \nbench_data_dir\n param and \nid\n is the ID of a particular benchmark.\n\n\nDefault value: \n\"metrics.txt\"\n\n\nbench_metrics_compression\n\n\n{bench_metrics_compression, (none|deflate)}\n\n\n\n\nEnable or disable metrics data compression with the \nDEFLATE\n algorithm.\n\n\nDefault value: \nnone\n\n\nnode_git\n\n\n{node_git, \nurl\n}\n\n\n\n\nThe MZBench git repository used to deploy worker nodes.\n\n\nBy default, the MZBench source code is taken from \nhttps://github.com/machinezone/mzbench.git\n.\n\n\nnode_commit\n\n\n{node_commit, \nstring\n}\n\n\n\n\nThe git commit SHA or branch name used to deploy worker nodes.\n\n\nBy default, the latest revision is used.\n\n\nnode_deployment_path\n\n\n{node_deployment_path, \npath\n}\n\n\n\n\nThe path to the MZBench installation on node machines.\n\n\nDefault value: \n\"/.local/share\"\n\n\nworker_deployment_path\n\n\n{worker_deployment_path, \npath\n}\n\n\n\n\nThe to the \nworkers\n installation on node machines.\n\n\nDefault value: \n\"~/.local/share/mzbench_workers\"\n\n\nplugins_dir\n\n\n{plugins_dir, \npath\n}\n\n\n\n\nDirectory with additional \ncloud plugins\n.\n\n\nDefault value: \n\"../../../../plugins\"\n\n\nbench_data_dir\n\n\n{bench_data_dir, \npath\n}\n\n\n\n\nThe location to store the data generated during the benchmark.\n\n\nDefault value: \n\"~/.local/share/mzbench_api/data\"\n.\n\n\ntgz_packages_dir\n\n\n{tgz_packages_dir, \npath\n}\n\n\n\n\nThe location to store prebuilt worker archives.\n\n\nDefault value: \n\"~/.local/cache/mzbench_api/packages\"\n.\n\n\nmax_bench_num\n\n\n{max_bench_num, \ninteger\n}\n\n\n\n\nMaximal number of benchmarks running at the same time.\n\n\nDefault value: \n1000\n.\n\n\nvm_args\n\n\n{vm_args, \nargs\n}\n\n\n\n\nAdditional arguments for the [Erlang VM](Additional arguments for the \nErlang VM\n.\n\n\nDefault value: \n[]\n.\n\n\nntp_max_timediff\n\n\n{ntp_max_timediff, \nfloat\n}\n\n\n\n\nMaximum timeout between node creation in seconds.\n\n\nThis check is optional and only prints a warning if fails.\n\n\nDefault value: \n0.1\n.\n\n\nDev Parameters\n\n\nSet these params only if you are an MZBench developer.\n\n\nbench_read_at_once\n\n\n{bench_read_at_once, \ninteger\n}\n\n\n\n\nThe number of bytes to read from the logs and metrics feed per request.\n\n\nDefault value: \n1024\n\n\nbench_poll_timeout\n\n\n{bench_poll_timeout, \ninteger\n}\n\n\n\n\nThe timeout between requests to logs and metrics feeds in milliseconds.\n\n\nDefault value: \n1000\n\n\nnode_log_port\n\n\n{node_log_port, \ninteger\n}\n\n\n\n\nThe TCP port for the logs feed.\n\n\nDefault value: \n4801\n.\n\n\nnode_management_port\n\n\n{node_management_port, \ninteger\n}\n\n\n\n\nThe TCP port used to control the server internally.\n\n\nDefault value: \n4802\n.", 
            "title": "Deployment"
        }, 
        {
            "location": "/deployment/#installation", 
            "text": "Requirements:   Erlang R17  C++ compiler (preinstalled on most UNIX-based systems)  Python  2.6 or 2.7  Pip  Python package manager   Install and start the MZBench API server:  # Clone the current MZBench source code:\ngit clone https://github.com/machinezone/mzbench.git\n\n# Install Python requirements:\nsudo pip install -r mzbench/requirements.txt\n\n# Start the server\n./mzbench/bin/mzbench start_server", 
            "title": "Installation"
        }, 
        {
            "location": "/deployment/#configuration", 
            "text": "Note  Every time you update the configuration,  restart the server .", 
            "title": "Configuration"
        }, 
        {
            "location": "/deployment/#format", 
            "text": "The MZBench server parameters are defined in the configuration file.  By default, the server tries to load configuration from  ~/.config/mzbench/server.config  and  /etc/mzbench/server.config .  To specify a configuration file from a different location, use the  --config  param:  $ ./bin/mzbench start_server --config /path/to/server.config  The configuration file is an Erlang list with the  mzbench_api  tuple. This tuple holds the list of the server  parameters :  [\n    {mzbench_api, [\n        {network_interface,  127.0.0.1 },\n        {listen_port, 80}\n    ]}\n].  Here, two parameters are specified:  network_interface  and  listen_port .  All parameters are optional.", 
            "title": "Format"
        }, 
        {
            "location": "/deployment/#parameters", 
            "text": "cloud_plugins  {cloud_plugins: [\n    { PluginName , #{module =   ModuleName ,\n                     Option1  =   Value1 ,\n                     Option2  =   Value2 ,\n                    ...\n                    }\n    },\n    ...\n    ]\n}  List of  cloud plugins  that can be used to allocate nodes. One plugin can be listed multiple times with different settings and under different names.  PluginName  is an atom identifying a plugin instance.  ModuleName  is the name of the plugin module. Each module has its specific  Options .   There are four built-in plugins:   mzb_api_ec2_plugin  Allocate hosts from the Amazon EC2 cloud.  mzb_staticcloud_plugin  Allocates hosts from a static pool.  mzb_dummycloud_plugin  Dummy provider, doesn t really do anything, but can be used as a reference during  cloud plugin development .  mzb_multicloud_plugin  Allocate hosts from multiple sources with the given ratio.   network_interface  {network_interface,  ip address }  Specify the IP address for the dashboard to run on. By default it s  \"127.0.0.1\" , so the dashboard is unavailable for external connections.   To open the dashboard to the public, set this param to  \"0.0.0.0\" .   Warning  MZBench provides no authentication. Opening the dashboard to the public makes your server vulnerable.  To protect your server, use an external auth proxy server like Nginx.   listen_port  {listen_port,  port }  Specify the port to access the dashboard.  Default value:  4800 .  bench_log_file  {bench_log_file,  filename }  The name of the benchmark log files. The files are stored in  bench_data_dir / bench_id  where  bench_data_dir  is defined in the  bench_data_dir  param and  id  is the ID of a particular benchmark.  Default value:  \"log.txt\"  bench_log_compression  {bench_log_compression, (deflate|none)}  Enable or disable log compression with the  DEFLATE  algorithm.  Default value:  deflate  bench_metrics_file  {bench_metrics_file,  filename }  The name of the benchmark metrics data files. The files are stored in  bench_data_dir / bench_id  where  bench_data_dir  is defined in the  bench_data_dir  param and  id  is the ID of a particular benchmark.  Default value:  \"metrics.txt\"  bench_metrics_compression  {bench_metrics_compression, (none|deflate)}  Enable or disable metrics data compression with the  DEFLATE  algorithm.  Default value:  none  node_git  {node_git,  url }  The MZBench git repository used to deploy worker nodes.  By default, the MZBench source code is taken from  https://github.com/machinezone/mzbench.git .  node_commit  {node_commit,  string }  The git commit SHA or branch name used to deploy worker nodes.  By default, the latest revision is used.  node_deployment_path  {node_deployment_path,  path }  The path to the MZBench installation on node machines.  Default value:  \"/.local/share\"  worker_deployment_path  {worker_deployment_path,  path }  The to the  workers  installation on node machines.  Default value:  \"~/.local/share/mzbench_workers\"  plugins_dir  {plugins_dir,  path }  Directory with additional  cloud plugins .  Default value:  \"../../../../plugins\"  bench_data_dir  {bench_data_dir,  path }  The location to store the data generated during the benchmark.  Default value:  \"~/.local/share/mzbench_api/data\" .  tgz_packages_dir  {tgz_packages_dir,  path }  The location to store prebuilt worker archives.  Default value:  \"~/.local/cache/mzbench_api/packages\" .  max_bench_num  {max_bench_num,  integer }  Maximal number of benchmarks running at the same time.  Default value:  1000 .  vm_args  {vm_args,  args }  Additional arguments for the [Erlang VM](Additional arguments for the  Erlang VM .  Default value:  [] .  ntp_max_timediff  {ntp_max_timediff,  float }  Maximum timeout between node creation in seconds.  This check is optional and only prints a warning if fails.  Default value:  0.1 .", 
            "title": "Parameters"
        }, 
        {
            "location": "/deployment/#dev-parameters", 
            "text": "Set these params only if you are an MZBench developer.  bench_read_at_once  {bench_read_at_once,  integer }  The number of bytes to read from the logs and metrics feed per request.  Default value:  1024  bench_poll_timeout  {bench_poll_timeout,  integer }  The timeout between requests to logs and metrics feeds in milliseconds.  Default value:  1000  node_log_port  {node_log_port,  integer }  The TCP port for the logs feed.  Default value:  4801 .  node_management_port  {node_management_port,  integer }  The TCP port used to control the server internally.  Default value:  4802 .", 
            "title": "Dev Parameters"
        }, 
        {
            "location": "/cloud_plugins/", 
            "text": "Built-in Cloud Plugins\n\n\nAmazon EC2\n\n\nmzb_api_ec2_plugin\n\n\nThe module allocates nodes in an Amazon EC2 cloud.\n\n\nThe AWS node images must have Erlang R17, gcc, gcc-c++, git, and sudo installed. Sudo must be available for non-tty execution; put \nDefaults !requiretty\n in \n/etc/sudoers\n. The SSH and TCP ports 4801 and 4802 must be open; MZBench uses them internally to send logs and metrics data from nodes to the server.\n\n\nThere\ns a ready-to-use Amazon Linux image with all necessary dependencies: \nami-3b90a80b\n. To use this image, specify it in the cloud plugin config as \nimage_id\n:\n\n\n{image_id, \nami-3b90a80b\n\n\n\n\n\nYou can, of course, build your own image based on the requirements listed above. \nLearn more\n in the official Amazon docs.\n\n\nConfiguration example:\n\n\n{cloud_plugins, [{ec2, #{module =\n mzb_api_ec2_plugin,\n                         instance_spec =\n [\n                          {image_id, \nami-3b90a80b\n},\n                          {group_set, \n},\n                          {key_name, \n-\n},\n                          {subnet_id, \n-\n},\n                          {instance_type, \nt2.micro\n},\n                          {availability_zone, \nus-west-2a\n}\n                        ],\n                        config =\n [\n                          {ec2_host, \nec2.us-west-2.amazonaws.com\n},\n                          {access_key_id, \n-\n},\n                          {secret_access_key, \n-\n}\n                         ]\n                        instance_user =\n \nec2-user\n,\n                    }}]},\n\n\n\n\nMinimally, the config requires \ninstance_spec\n and \nconfig\n keys specified. Learn more about AWS-specific config in the \nerlcloud documentation\n.\n\n\nStatic Cloud\n\n\nmzb_staticcloud_plugin\n\n\nThe module allocated nodes from a list of hosts.\n\n\nConfiguration example:\n\n\n{cloud_plugins, [{static, #{module =\n mzb_staticcloud_plugin,\n                           hosts =\n [\n123.45.67.89\n, \nhostname\n]\n                           }}]}\n\n\n\n\nDummy\n\n\nmzb_dummycloud_plugin\n\n\nThe module does not allocate any hosts; localhost is used instead.\n\n\nConfiguration example:\n\n\n{cloud_plugins, [{dummy, #{module =\n mzb_dummycloud_plugin}}]}\n\n\n\n\nMulticloud\n\n\nmzb_multicloud_plugin\n\n\nCombine multiple plugins to allocate hosts from multiple sources.\n\n\nHow to Write a Cloud Plugin\n\n\nCloud plugin is an Erlang module with at least three methods:\n\n\n-spec start(Name, Opts) -\n PluginRef when\n    Name :: atom(),\n    Opts :: #{},\n    PluginRef :: term().\n\n-spec create_cluster(PluginRef, NumNodes, Config) -\n {ok, ClusterID, UserName, [Host]} when\n    PluginRef :: term(),\n    NumNodes :: pos_integer(),\n    Config :: #{},\n    ClusterID :: term()\n    UserName :: string(),\n    Host :: string().\n\n-spec destroy_cluster(ClusterID) -\n ok when\n    ClusterID :: term().\n\n\n\n\n\n\nstart\n\n\n\n\nStart a particular instance of the plugin and get an instance reference.\n\n\n\n\nName\n\n\nThe name of the particular instance of the plugin specified in the \nconfiguration file\n.\n\n\nOpts\n\n\nOptions passed from the server \nconfiguration file\n for the particular plugin instance.\n\n\n\n\n\n\ncreate_cluster\n\n\n\n\nAllocate the required number of nodes and return a tuple: \n{ok, ClusterID, UserName, HostList}\n.   \n\n\n\n\nNumNodes\n\n\nNumber of nodes to allocate.\n\n\nConfig\n\n\nMap with keys \nuser\n, \nname\n, \ndescription\n, and \nexclusive_node_usage\n.\n\n\nClusterID\n\n\nThis term will be passed to \ndestroy_cluster/1\n when it\ns time it deallocate the nodes. Its content is up to the plugin developer.\n\n\nUserName\n\n\nSSH username to connect to the allocated nodes.\n\n\nHostList\n\n\nList of hostnames or IPs of the allocated nodes.\n\n\n\n\n\n\ndestroy_cluster\n\n\n\n\nDeallocate the required number of nodes and return \nok\n.\n\n\n\n\nClusterID\n\n\nTerm returned by \ncreate_cluster/1\n.\n\n\n\n\n\n\n\n\nUsing the Cloud Plugin\n\n\nSpecify the \nplugin module\n and the \npath to it\n in the MZBench config file in the \nmzbench_api\n section:\n\n\n    [\n      {mzbench_api, [\n        {cloud_plugins, [{my_cloud1, #{module =\n mycloud_plugin,\n                                       Opt1 =\n Value1,\n                                       Opt2 =\n Value2, ...}},\n                         ...\n                        ]},\n        {plugins_dir, \n/path/to/my/plugin\n}\n        ]},\n    ].\n\n\n\n\nThe plugin binaries must be placed in a subdirectory inside \nplugins_dir\n, e.g. \n/mycloud-0.1.1/ebin/mycloud.ebin\n.\n\n\n\n\nNote\n\n\nPlugin will be started using \napplication:ensure_all_started/1\n just before the benchmark start.", 
            "title": "Cloud Plugins"
        }, 
        {
            "location": "/cloud_plugins/#built-in-cloud-plugins", 
            "text": "", 
            "title": "Built-in Cloud Plugins"
        }, 
        {
            "location": "/cloud_plugins/#amazon-ec2", 
            "text": "mzb_api_ec2_plugin  The module allocates nodes in an Amazon EC2 cloud.  The AWS node images must have Erlang R17, gcc, gcc-c++, git, and sudo installed. Sudo must be available for non-tty execution; put  Defaults !requiretty  in  /etc/sudoers . The SSH and TCP ports 4801 and 4802 must be open; MZBench uses them internally to send logs and metrics data from nodes to the server.  There s a ready-to-use Amazon Linux image with all necessary dependencies:  ami-3b90a80b . To use this image, specify it in the cloud plugin config as  image_id :  {image_id,  ami-3b90a80b   You can, of course, build your own image based on the requirements listed above.  Learn more  in the official Amazon docs.  Configuration example:  {cloud_plugins, [{ec2, #{module =  mzb_api_ec2_plugin,\n                         instance_spec =  [\n                          {image_id,  ami-3b90a80b },\n                          {group_set,  },\n                          {key_name,  - },\n                          {subnet_id,  - },\n                          {instance_type,  t2.micro },\n                          {availability_zone,  us-west-2a }\n                        ],\n                        config =  [\n                          {ec2_host,  ec2.us-west-2.amazonaws.com },\n                          {access_key_id,  - },\n                          {secret_access_key,  - }\n                         ]\n                        instance_user =   ec2-user ,\n                    }}]},  Minimally, the config requires  instance_spec  and  config  keys specified. Learn more about AWS-specific config in the  erlcloud documentation .", 
            "title": "Amazon EC2"
        }, 
        {
            "location": "/cloud_plugins/#static-cloud", 
            "text": "mzb_staticcloud_plugin  The module allocated nodes from a list of hosts.  Configuration example:  {cloud_plugins, [{static, #{module =  mzb_staticcloud_plugin,\n                           hosts =  [ 123.45.67.89 ,  hostname ]\n                           }}]}", 
            "title": "Static Cloud"
        }, 
        {
            "location": "/cloud_plugins/#dummy", 
            "text": "mzb_dummycloud_plugin  The module does not allocate any hosts; localhost is used instead.  Configuration example:  {cloud_plugins, [{dummy, #{module =  mzb_dummycloud_plugin}}]}", 
            "title": "Dummy"
        }, 
        {
            "location": "/cloud_plugins/#multicloud", 
            "text": "mzb_multicloud_plugin  Combine multiple plugins to allocate hosts from multiple sources.", 
            "title": "Multicloud"
        }, 
        {
            "location": "/cloud_plugins/#how-to-write-a-cloud-plugin", 
            "text": "Cloud plugin is an Erlang module with at least three methods:  -spec start(Name, Opts) -  PluginRef when\n    Name :: atom(),\n    Opts :: #{},\n    PluginRef :: term().\n\n-spec create_cluster(PluginRef, NumNodes, Config) -  {ok, ClusterID, UserName, [Host]} when\n    PluginRef :: term(),\n    NumNodes :: pos_integer(),\n    Config :: #{},\n    ClusterID :: term()\n    UserName :: string(),\n    Host :: string().\n\n-spec destroy_cluster(ClusterID) -  ok when\n    ClusterID :: term().   start   Start a particular instance of the plugin and get an instance reference.   Name  The name of the particular instance of the plugin specified in the  configuration file .  Opts  Options passed from the server  configuration file  for the particular plugin instance.    create_cluster   Allocate the required number of nodes and return a tuple:  {ok, ClusterID, UserName, HostList} .      NumNodes  Number of nodes to allocate.  Config  Map with keys  user ,  name ,  description , and  exclusive_node_usage .  ClusterID  This term will be passed to  destroy_cluster/1  when it s time it deallocate the nodes. Its content is up to the plugin developer.  UserName  SSH username to connect to the allocated nodes.  HostList  List of hostnames or IPs of the allocated nodes.    destroy_cluster   Deallocate the required number of nodes and return  ok .   ClusterID  Term returned by  create_cluster/1 .", 
            "title": "How to Write a Cloud Plugin"
        }, 
        {
            "location": "/cloud_plugins/#using-the-cloud-plugin", 
            "text": "Specify the  plugin module  and the  path to it  in the MZBench config file in the  mzbench_api  section:      [\n      {mzbench_api, [\n        {cloud_plugins, [{my_cloud1, #{module =  mycloud_plugin,\n                                       Opt1 =  Value1,\n                                       Opt2 =  Value2, ...}},\n                         ...\n                        ]},\n        {plugins_dir,  /path/to/my/plugin }\n        ]},\n    ].  The plugin binaries must be placed in a subdirectory inside  plugins_dir , e.g.  /mycloud-0.1.1/ebin/mycloud.ebin .   Note  Plugin will be started using  application:ensure_all_started/1  just before the benchmark start.", 
            "title": "Using the Cloud Plugin"
        }, 
        {
            "location": "/workers/", 
            "text": "Today internet is a huge set of ever changing technologies. It would be impossible in practice for MZBench distribution to provide an extensive library of functions to access all the possible services and protocols. Instead, it uses a plugin system called \nworkers\n.\n\n\nWorker\n is an Erlang application providing \nstatements\n to access a particular service (such as HTTP or SSH server) and collecting statistics about its usage. Some workers are bundled with the distribution, but you will likely need to write your own to access the service you want to benchmark. This document is here to guide you through this process.\n\n\nBecause a worker is an Erlang application, you need basic knowledge of this programming language to understand this document. Refer to \nGetting Started with Erlang User\ns Guide\n or to the \nLearn You Some Erlang for great good!\n book for an introduction to the matter.\n\n\nHow to Write a Worker\n\n\nCommand Line Utilities\n\n\nMZBench distribution provides \ncommand line utilities\n to assist you during your development effort.\n\n\n\n\nNote\n\n\nAll command examples below are executed in the MZBench directory. To run them from a different location, specify the full path to MZBench:\n\n\n$ /path/to/mzbench/bin/mzbench\n\n\n\n\n\nGenerate\n\n\nFirst of all, generate an empty worker application with \nnew_worker\n:\n\n\n$ ./bin/mzbench new_worker \nworker_name\n\n\n\n\n\nIt creates a new directory \nworker_name\n with a minimalistic but fully functional MZBench worker named \nworker_name\n. Particularly interesting files are \nsrc/\nworker_name\n.erl\n, which holds the worker source code, and \nexamples/\nworker_name\n.erl\n, which contains a simple MZBench \nscenario\n using it.\n\n\nIf the service you develop a worker for is based on a well-known protocol like TCP, the \nnew_worker\n command can generate your a more elaborate worker already containing the usual boilerplate code for this type of service. List available protocol templates with \nlist_templates\n:\n\n\n$ ./bin/mzbench list_templates\n\n\n\n\nThen generate your worker with the additional \n--template\n parameter:\n\n\n./bin/mzbench new_worker --template \nprotocol\n \nworker_name\n\n\n\n\n\nCompile and Debug\n\n\nDuring development, you\nll need to do a lot of debugging. If you had to launch a complete benchmark every time you want to launch your test scenario, it would be cumbersome. MZBench provides a quicker way to build your worker and launch a local instance of your benchmarking scenario using it.\n\n\nIn the worker directory, run \nrun_local \nscript\n, where \nscript\n is the path to the scenario to run:\n\n\n$ ./bin/mzbench run_local \nscript\n\n\n\n\n\nYou can define environment variables with the \n--env\n option.\n\n\n\n\nNote\n\n\nAll \nmake_install\n top-level statements are ignored in this execution mode.\n\n\n\n\nExecute\n\n\nAfter you have done with the debugging, execute your worker in a cloud: Specify the worker git address in your benchmark scenario with \n{make_install, [{git, \nURL\n}, {branch, \nBranch\n}, {dir, \nDir\n}]}\n.\n\n\nSimple HTTP worker example \u2192\n\n\nGeneral Worker Structure\n\n\nA worker provides DSL statements and metrics. The statements need not to be independent as the worker can have internal state.\n\n\nTo understand the general structure of a worker, let\ns see the source code of the \ndummy_worker\n provided with the MZBench distribution:\n\n\n-module(dummy_worker).\n-export([initial_state/0, metrics/0,\n         print/3]).\n\n-include(\nmzb_types.hrl\n).\n\n-type state() :: string().\n-type meta() :: [{Key :: atom(), Value :: any()}].\n\n-type graph_group() :: {group, Name :: string(), [graph()]}\n                     | graph().\n-type graph()       :: {graph, Opts :: #{metrics =\n [metric()],\n                                         units =\n string(),\n                                         title =\n string()}}\n                     | [metric()]\n                     | metric().\n-type metric()      :: {Name :: string(), Type :: metric_type() }\n                     | {Name :: string(), Type :: metric_type(), Opts :: map() }.\n-type metric_type() :: counter | gauge | histogram.\n\n-spec initial_state() -\n state().\ninitial_state() -\n [].\n\n-spec metrics() -\n [graph_group()].\nmetrics() -\n [{group, \nApplication Metrics\n, [\n                {graph, #{ title =\n \nDummy counter\n,\n                           units =\n \nbudger\n,\n                           metrics =\n [{\ndummy_counter\n, counter}]}}\n             ]}].\n\n-spec print(state(), meta(), string()) -\n {nil, state()}.\nprint(State, Meta, Text) -\n\n    mzb_metrics:notify({\ndummy_counter\n, counter}, 1),\n    lager:info(\nPrinting ~p, Meta: ~p~n\n, [Text, Meta]),\n    {nil, State}.\n\n\n\n\nIt exports three functions: \ninitial_state/0\n, \nmetrics/0\n, and \nprint/3\n. The first two are mandatory for any worker. \n\n\n\n\ninitial_state/0\n\n\nSet the worker\ns initial state. Each parallel job has its own state, so this function will be called once per job start.\n\n\nmetrics/0\n\n\nReturn a group of metrics generated by this worker. \nHow to define metrics\n.\n\n\n\n\nThe rest of the exported functions define the DSL statements provided by this worker. You can, of course, provide none, although such a worker wouldn\nt be very useful. The \ndummy_worker\n, for instance, provides the \nprint\n statement to output a string to the standard output. \nHow to define statements\n.\n\n\nHow to Define Statements\n\n\nTo define a DSL statement provided by your worker, export an Erlang function that will be called when this statement is encountered:\n\n\nstatement_name\n(State, Meta, [\nParam1\n, [\nParam2\n, ...]]) -\n\n    {ReturnValue, NewState}.\n\n\n\n\nThe function has the same name as the statement it defines. It accepts at least two parameters: the worker internal state at the moment the statement is executed and \nmeta\n information proplist. The function can also accept any number of other parameters. They correspond to the parameters of the statement.\n\n\nFor example, this function:\n\n\nfoo(State, Meta, X, Y) -\n\n    {nil, State}.\n\n\n\n\nis called as \n{foo, X, Y}\n from a benchmarking scenario.\n\n\nThe statement function must return a tuple of two values:\n\n\n\n\nthe return value of statement; return \nnil\n if your statement has no return value\n\n\nthe next worker state\n\n\n\n\nStatements are processed sequentially; each statement receives the state from the previous one and passes it further.\n\n\nTwo exceptions are the statements within the \n{parallel}\n section and iterations within a \n{loop}\n with \nparallel \n 1\n. In these cases the statements within the same thread share the same sequence of statements, which parallel threads don\nt. The final state of the whole \n{parallel}\n or \n{loop}\n statement is the one from the first \nthread\n; other threads\n states don\nt affect the final state.\n\n\nMetrics\n\n\nMetrics are numerical values collected during the scenario execution. They are the main result of your worker and represent the values you want to evaluate with your benchmark.\n\n\nMetric Types\n\n\nMZBench currently support four types of metrics:\n\n\n\n\ncounter\n\n\nA single additive value. New values are simply added to the current one.\n\n\ngauge\n\n\nA single non-additive value. New value replaces the previous one.\n\n\nhistogram\n\n\nA set of numerical values that quantify a distribution of values. New values are added to the distribution.\n\n\nderived\n\n\nEvaluated periodically using user-defined function based on another metric values. \nLearn more\n.\n\n\n\n\nFor example, if you are consuming TCP packets of various sizes and you want to track overall amount of data being transferred, use \ncounter\n. If you are interested in its distribution\u2013mean size, 50 percentile, and so on\u2013you need a \nhistogram\n.\n\n\nDeclaring Metrics\n\n\nDeclare the groups of metrics collected by your worker in the list returned by \nmetrics/0\n. Each group corresponds to a structure with following spec:\n\n\ngraph_group() :: {group, Name :: string(), [graph()]}\n               | graph().\ngraph()       :: {graph, Opts :: #{metrics =\n [metric()],\n                                   units =\n string(),\n                                   title =\n string()}}\n               | [metric()]\n               | metric().\nmetric()      :: {Name :: string(), Type :: metric_type() }\n               | {Name :: string(), Type :: metric_type(), Opts :: map()}.\nmetric_type() :: counter | gauge | histogram.\n\n\n\n\nThis structure has a three-level hierarchy:\n\n\n\n\nGroup of graphs is placed on the top of this hierarchy. It consists of one or more graphs and defines a group of graphs under the same name.\n\n\nGraph consists of one or more metrics that will be plotted on the same chart. Furthermore, you could specify additional options for the chart: units, title, etc.\n\n\nMetric is the lowest unit of this hierarchy. It specifies the name and type of the user-defined metric.\n\n\n\n\nLet\ns see the following metrics declaration:\n\n\nmetrics() -\n [{group, \nHTTP Requests\n, [\n                {graph, #{metrics =\n [{\nsuccess_requests\n, counter}, {\nfailed_requests\n, counter}]}},\n                {graph, #{title =\n \nRequest's latency\n,\n                          units =\n \nms\n,\n                          metrics =\n [{\nlatency\n, histogram}]}}]}].\n\n\n\n\nIn this example, a group of graphs with the name \nHTTP Requests\n is created. It consists of several graphs representing the number of successful and failed requests and the request latencies.\n\n\nA graph can produce several charts. In the example above, the graph for successful and failed request produces two charts: absolute counters and their rps.\n\n\nDerived Metrics\n\n\nDerived metrics are basically gauges which are evaluated on the director node every ~10sec. To define a derived metric, specify the \nresolver\n function in the metric opts dictionary. This function is used to evaluate the metric value.\n\n\nTypical example of a derived metric is the current number of pending requests. We specify a function (\npending_requests\n) to calculate the metric value in the metric options and then define the function as simple difference between the number of sent requests and received responses:\n\n\nmetrics() -\n [{group, \nRequests\n, [\n                {graph, #{metrics =\n [\n                    {\nrequests_sent\n, counter},\n                    {\nresponses_received\n, counter},\n                    {\npending_requests\n, derived, #{resolver =\n pending_requests}}]}},\n                ]}].\n\npending_requests() -\n\n    mzb_metrics:get_value(\nrequests_sent\n) - mzb_metrics:get_value(\nresponses_received\n).\n\n\n\n\nHooks\n\n\nPre and post hooks\n let you run custom code before and after a benchmark. Hooks can be applied on every node or only on the director node. You can change any environment variable in your hook handler and use it in your scenario.\n\n\nScenario:\n\n\n{pre_hook, [\n    {exec, all, \nyum install zlib\n},\n    {worker_call, fetch_commit, my_worker}\n]}\n\n{pool, [{size, 3}, {worker_type, dummy_worker}], [\n    {loop, [{time, {1, sec}},\n            {rate, {ramp, linear, {10, rps}, {50, rps}}}],\n        [{print, {var, \ncommit\n, \ndefault\n}}]}]},\n\n\n\n\nWorker:\n\n\nfetch_commit(Env) -\n\n    {ok, [{\ncommit\n, \n0123456\n} | Env]}.\n\n\n\n\nUpdating Metrics\n\n\nYou can update a metric from anywhere inside your worker. Simply call the following function:\n\n\nmzb_metrics:notify({\nmetric_name\n, \nmetric_type\n}, \nvalue\n)\n\n\n\n\nThe tuple \n{\"\nmetric_name\n\", \nmetric_type\n}\n is the same that was used during the metric declaration and identifies the metric to update. \nvalue\n is the value to add to the metric.", 
            "title": "Workers"
        }, 
        {
            "location": "/workers/#how-to-write-a-worker", 
            "text": "", 
            "title": "How to Write a Worker"
        }, 
        {
            "location": "/workers/#command-line-utilities", 
            "text": "MZBench distribution provides  command line utilities  to assist you during your development effort.   Note  All command examples below are executed in the MZBench directory. To run them from a different location, specify the full path to MZBench:  $ /path/to/mzbench/bin/mzbench   Generate  First of all, generate an empty worker application with  new_worker :  $ ./bin/mzbench new_worker  worker_name   It creates a new directory  worker_name  with a minimalistic but fully functional MZBench worker named  worker_name . Particularly interesting files are  src/ worker_name .erl , which holds the worker source code, and  examples/ worker_name .erl , which contains a simple MZBench  scenario  using it.  If the service you develop a worker for is based on a well-known protocol like TCP, the  new_worker  command can generate your a more elaborate worker already containing the usual boilerplate code for this type of service. List available protocol templates with  list_templates :  $ ./bin/mzbench list_templates  Then generate your worker with the additional  --template  parameter:  ./bin/mzbench new_worker --template  protocol   worker_name   Compile and Debug  During development, you ll need to do a lot of debugging. If you had to launch a complete benchmark every time you want to launch your test scenario, it would be cumbersome. MZBench provides a quicker way to build your worker and launch a local instance of your benchmarking scenario using it.  In the worker directory, run  run_local  script , where  script  is the path to the scenario to run:  $ ./bin/mzbench run_local  script   You can define environment variables with the  --env  option.   Note  All  make_install  top-level statements are ignored in this execution mode.   Execute  After you have done with the debugging, execute your worker in a cloud: Specify the worker git address in your benchmark scenario with  {make_install, [{git,  URL }, {branch,  Branch }, {dir,  Dir }]} .  Simple HTTP worker example \u2192", 
            "title": "Command Line Utilities"
        }, 
        {
            "location": "/workers/#general-worker-structure", 
            "text": "A worker provides DSL statements and metrics. The statements need not to be independent as the worker can have internal state.  To understand the general structure of a worker, let s see the source code of the  dummy_worker  provided with the MZBench distribution:  -module(dummy_worker).\n-export([initial_state/0, metrics/0,\n         print/3]).\n\n-include( mzb_types.hrl ).\n\n-type state() :: string().\n-type meta() :: [{Key :: atom(), Value :: any()}].\n\n-type graph_group() :: {group, Name :: string(), [graph()]}\n                     | graph().\n-type graph()       :: {graph, Opts :: #{metrics =  [metric()],\n                                         units =  string(),\n                                         title =  string()}}\n                     | [metric()]\n                     | metric().\n-type metric()      :: {Name :: string(), Type :: metric_type() }\n                     | {Name :: string(), Type :: metric_type(), Opts :: map() }.\n-type metric_type() :: counter | gauge | histogram.\n\n-spec initial_state() -  state().\ninitial_state() -  [].\n\n-spec metrics() -  [graph_group()].\nmetrics() -  [{group,  Application Metrics , [\n                {graph, #{ title =   Dummy counter ,\n                           units =   budger ,\n                           metrics =  [{ dummy_counter , counter}]}}\n             ]}].\n\n-spec print(state(), meta(), string()) -  {nil, state()}.\nprint(State, Meta, Text) - \n    mzb_metrics:notify({ dummy_counter , counter}, 1),\n    lager:info( Printing ~p, Meta: ~p~n , [Text, Meta]),\n    {nil, State}.  It exports three functions:  initial_state/0 ,  metrics/0 , and  print/3 . The first two are mandatory for any worker.    initial_state/0  Set the worker s initial state. Each parallel job has its own state, so this function will be called once per job start.  metrics/0  Return a group of metrics generated by this worker.  How to define metrics .   The rest of the exported functions define the DSL statements provided by this worker. You can, of course, provide none, although such a worker wouldn t be very useful. The  dummy_worker , for instance, provides the  print  statement to output a string to the standard output.  How to define statements .", 
            "title": "General Worker Structure"
        }, 
        {
            "location": "/workers/#how-to-define-statements", 
            "text": "To define a DSL statement provided by your worker, export an Erlang function that will be called when this statement is encountered:  statement_name (State, Meta, [ Param1 , [ Param2 , ...]]) - \n    {ReturnValue, NewState}.  The function has the same name as the statement it defines. It accepts at least two parameters: the worker internal state at the moment the statement is executed and  meta  information proplist. The function can also accept any number of other parameters. They correspond to the parameters of the statement.  For example, this function:  foo(State, Meta, X, Y) - \n    {nil, State}.  is called as  {foo, X, Y}  from a benchmarking scenario.  The statement function must return a tuple of two values:   the return value of statement; return  nil  if your statement has no return value  the next worker state   Statements are processed sequentially; each statement receives the state from the previous one and passes it further.  Two exceptions are the statements within the  {parallel}  section and iterations within a  {loop}  with  parallel   1 . In these cases the statements within the same thread share the same sequence of statements, which parallel threads don t. The final state of the whole  {parallel}  or  {loop}  statement is the one from the first  thread ; other threads  states don t affect the final state.", 
            "title": "How to Define Statements"
        }, 
        {
            "location": "/workers/#metrics", 
            "text": "Metrics are numerical values collected during the scenario execution. They are the main result of your worker and represent the values you want to evaluate with your benchmark.  Metric Types  MZBench currently support four types of metrics:   counter  A single additive value. New values are simply added to the current one.  gauge  A single non-additive value. New value replaces the previous one.  histogram  A set of numerical values that quantify a distribution of values. New values are added to the distribution.  derived  Evaluated periodically using user-defined function based on another metric values.  Learn more .   For example, if you are consuming TCP packets of various sizes and you want to track overall amount of data being transferred, use  counter . If you are interested in its distribution\u2013mean size, 50 percentile, and so on\u2013you need a  histogram .  Declaring Metrics  Declare the groups of metrics collected by your worker in the list returned by  metrics/0 . Each group corresponds to a structure with following spec:  graph_group() :: {group, Name :: string(), [graph()]}\n               | graph().\ngraph()       :: {graph, Opts :: #{metrics =  [metric()],\n                                   units =  string(),\n                                   title =  string()}}\n               | [metric()]\n               | metric().\nmetric()      :: {Name :: string(), Type :: metric_type() }\n               | {Name :: string(), Type :: metric_type(), Opts :: map()}.\nmetric_type() :: counter | gauge | histogram.  This structure has a three-level hierarchy:   Group of graphs is placed on the top of this hierarchy. It consists of one or more graphs and defines a group of graphs under the same name.  Graph consists of one or more metrics that will be plotted on the same chart. Furthermore, you could specify additional options for the chart: units, title, etc.  Metric is the lowest unit of this hierarchy. It specifies the name and type of the user-defined metric.   Let s see the following metrics declaration:  metrics() -  [{group,  HTTP Requests , [\n                {graph, #{metrics =  [{ success_requests , counter}, { failed_requests , counter}]}},\n                {graph, #{title =   Request's latency ,\n                          units =   ms ,\n                          metrics =  [{ latency , histogram}]}}]}].  In this example, a group of graphs with the name  HTTP Requests  is created. It consists of several graphs representing the number of successful and failed requests and the request latencies.  A graph can produce several charts. In the example above, the graph for successful and failed request produces two charts: absolute counters and their rps.  Derived Metrics  Derived metrics are basically gauges which are evaluated on the director node every ~10sec. To define a derived metric, specify the  resolver  function in the metric opts dictionary. This function is used to evaluate the metric value.  Typical example of a derived metric is the current number of pending requests. We specify a function ( pending_requests ) to calculate the metric value in the metric options and then define the function as simple difference between the number of sent requests and received responses:  metrics() -  [{group,  Requests , [\n                {graph, #{metrics =  [\n                    { requests_sent , counter},\n                    { responses_received , counter},\n                    { pending_requests , derived, #{resolver =  pending_requests}}]}},\n                ]}].\n\npending_requests() - \n    mzb_metrics:get_value( requests_sent ) - mzb_metrics:get_value( responses_received ).  Hooks  Pre and post hooks  let you run custom code before and after a benchmark. Hooks can be applied on every node or only on the director node. You can change any environment variable in your hook handler and use it in your scenario.  Scenario:  {pre_hook, [\n    {exec, all,  yum install zlib },\n    {worker_call, fetch_commit, my_worker}\n]}\n\n{pool, [{size, 3}, {worker_type, dummy_worker}], [\n    {loop, [{time, {1, sec}},\n            {rate, {ramp, linear, {10, rps}, {50, rps}}}],\n        [{print, {var,  commit ,  default }}]}]},  Worker:  fetch_commit(Env) - \n    {ok, [{ commit ,  0123456 } | Env]}.  Updating Metrics  You can update a metric from anywhere inside your worker. Simply call the following function:  mzb_metrics:notify({ metric_name ,  metric_type },  value )  The tuple  {\" metric_name \",  metric_type }  is the same that was used during the metric declaration and identifies the metric to update.  value  is the value to add to the metric.", 
            "title": "Metrics"
        }
    ]
}